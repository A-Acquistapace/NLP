{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "0MAiXgYbAVLU",
    "outputId": "45368e3d-b374-42ba-d6a4-443ac0e9b569"
   },
   "source": [
    "!pip install --upgrade tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2qUhyNPnhBtk",
    "outputId": "249b5b52-0f6e-43f5-a873-4a4d20fe67b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import des librairies nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSWse8KgHmQw"
   },
   "source": [
    "## Import des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U2-Sd6lq_8ax"
   },
   "outputs": [],
   "source": [
    "# Fonction de chargement du document txt\n",
    "def load_doc(url):\n",
    "    df = pd.read_csv(\"https://go.aws/38ECHUB\", delimiter=\"\\t\", header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "Yj34GLiihGw1",
    "outputId": "c7ec1440-8799-443e-c96f-ae80d4b8d746"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0           1\n",
       "0   Go.        Va !\n",
       "1   Hi.     Salut !\n",
       "2  Run!     Cours !\n",
       "3  Run!    Courez !\n",
       "4  Wow!  Ça alors !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement du document txt\n",
    "doc = load_doc(\"https://go.aws/38ECHUB\")\n",
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YrcFSfBZMuQ7"
   },
   "outputs": [],
   "source": [
    "# Prenons simplement un sample de 5000 phrases pour éviter un temps de traitement trop long. Si votre machine dépote, on peut bien sûr augmenter sa taille\n",
    "doc = doc.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Z1Ih3M2jVr_"
   },
   "outputs": [],
   "source": [
    "# Add a <start> and <end> token \n",
    "def begin_end_sentence(sentence):\n",
    "    sentence = \"<start> \"+ sentence + \" <end>\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XVMl6744jmrq"
   },
   "outputs": [],
   "source": [
    "# Add <start> and <end> token\n",
    "doc.iloc[:, 0] = doc.iloc[:, 0].apply(lambda x: begin_end_sentence(x))\n",
    "#doc.iloc[:, 1] = doc.iloc[:, 1].apply(lambda x: begin_end_sentence(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "E0gGTgDFhJN6",
    "outputId": "2ef90be9-482c-4ea7-ed43-0f26b4f97f86"
   },
   "source": [
    "# Chargement des langages français et anglais de spacy \n",
    "!python -m spacy download fr_core_news_md\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cBPt41Isna8a"
   },
   "outputs": [],
   "source": [
    "# Import de chacun des langages\n",
    "import fr_core_news_sm\n",
    "import en_core_web_sm\n",
    "nlp_fr = fr_core_news_sm.load()\n",
    "nlp_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PE7sn2ftlfJU"
   },
   "outputs": [],
   "source": [
    "# Add <start> & <end> special case\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "start_case = [{ORTH:\"<start>\"}]\n",
    "end_case = [{ORTH: \"<end>\"}]\n",
    "\n",
    "#nlp_fr.tokenizer.add_special_case(\"<start>\", start_case)\n",
    "#nlp_fr.tokenizer.add_special_case(\"<end>\", end_case)\n",
    "\n",
    "nlp_en.tokenizer.add_special_case(\"<start>\", start_case)\n",
    "nlp_en.tokenizer.add_special_case(\"<end>\", end_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2O4hj8DGoFzr"
   },
   "outputs": [],
   "source": [
    "# Chargement du corpus entier de phrases françaises et anglaises\n",
    "fr_corpus = \" \".join(doc.iloc[:, 1].to_list())\n",
    "en_corpus = \" \".join(doc.iloc[:, 0].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "DPzvDSUWoWfW",
    "outputId": "a1708c75-72c6-4d33-c46d-79344753180b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "# Chargement des deux corpus dans spacy \n",
    "\n",
    "nlp_fr.max_length = len(fr_corpus)\n",
    "nlp_en.max_length = len(en_corpus)\n",
    "\n",
    "fr_doc = nlp_fr(fr_corpus)\n",
    "en_doc = nlp_en(en_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fr_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5250b75da86e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfr_doc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fr_doc' is not defined"
     ]
    }
   ],
   "source": [
    "fr_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "rBu43uJ77Ad6",
    "outputId": "e7f54e9f-f0bc-4bec-864d-54df725ea0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenisation de chacune des phrases via spacy \n",
    "\n",
    "doc[\"fr_tokens\"] = doc.iloc[:, 1].apply(lambda x: nlp_fr.tokenizer(x))\n",
    "doc[\"en_tokens\"] = doc.iloc[:, 0].apply(lambda x: nlp_en.tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "bQQOU7OI7-xV",
    "outputId": "752b7034-9847-4869-e206-e8c63c1969e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>fr_tokens</th>\n",
       "      <th>en_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97878</th>\n",
       "      <td>&lt;start&gt; Are you going to eat those eggs? &lt;end&gt;</td>\n",
       "      <td>Vas-tu manger ces œufs ?</td>\n",
       "      <td>(Vas, -, tu, manger, ces, œufs,  , ?)</td>\n",
       "      <td>(&lt;start&gt;, Are, you, going, to, eat, those, egg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40227</th>\n",
       "      <td>&lt;start&gt; Marriage is a lottery. &lt;end&gt;</td>\n",
       "      <td>Le mariage est une loterie.</td>\n",
       "      <td>(Le, mariage, est, une, loterie, .)</td>\n",
       "      <td>(&lt;start&gt;, Marriage, is, a, lottery, ., &lt;end&gt;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135212</th>\n",
       "      <td>&lt;start&gt; They're expressing their love by huggi...</td>\n",
       "      <td>Ils expriment leur amour en s'enlaçant.</td>\n",
       "      <td>(Ils, expriment, leur, amour, en, s', enlaçant...</td>\n",
       "      <td>(&lt;start&gt;, They, 're, expressing, their, love, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78744</th>\n",
       "      <td>&lt;start&gt; Tom and Mary know John lied. &lt;end&gt;</td>\n",
       "      <td>Tom et Mary savent que John a menti.</td>\n",
       "      <td>(Tom, et, Mary, savent, que, John, a, menti, .)</td>\n",
       "      <td>(&lt;start&gt;, Tom, and, Mary, know, John, lied, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24790</th>\n",
       "      <td>&lt;start&gt; Tom led the attack. &lt;end&gt;</td>\n",
       "      <td>Tom a dirigé l'offensive.</td>\n",
       "      <td>(Tom, a, dirigé, l', offensive, .)</td>\n",
       "      <td>(&lt;start&gt;, Tom, led, the, attack, ., &lt;end&gt;)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0  \\\n",
       "97878      <start> Are you going to eat those eggs? <end>   \n",
       "40227                <start> Marriage is a lottery. <end>   \n",
       "135212  <start> They're expressing their love by huggi...   \n",
       "78744          <start> Tom and Mary know John lied. <end>   \n",
       "24790                   <start> Tom led the attack. <end>   \n",
       "\n",
       "                                              1  \\\n",
       "97878                  Vas-tu manger ces œufs ?   \n",
       "40227               Le mariage est une loterie.   \n",
       "135212  Ils expriment leur amour en s'enlaçant.   \n",
       "78744      Tom et Mary savent que John a menti.   \n",
       "24790                 Tom a dirigé l'offensive.   \n",
       "\n",
       "                                                fr_tokens  \\\n",
       "97878               (Vas, -, tu, manger, ces, œufs,  , ?)   \n",
       "40227                 (Le, mariage, est, une, loterie, .)   \n",
       "135212  (Ils, expriment, leur, amour, en, s', enlaçant...   \n",
       "78744     (Tom, et, Mary, savent, que, John, a, menti, .)   \n",
       "24790                  (Tom, a, dirigé, l', offensive, .)   \n",
       "\n",
       "                                                en_tokens  \n",
       "97878   (<start>, Are, you, going, to, eat, those, egg...  \n",
       "40227       (<start>, Marriage, is, a, lottery, ., <end>)  \n",
       "135212  (<start>, They, 're, expressing, their, love, ...  \n",
       "78744   (<start>, Tom, and, Mary, know, John, lied, .,...  \n",
       "24790          (<start>, Tom, led, the, attack, ., <end>)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CwCN5z21xo5H",
    "outputId": "9fd66835-ba0d-48d3-a065-0373dc143ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8262\n"
     ]
    }
   ],
   "source": [
    "# Création d'un set() qui va prendre tous les tokens unique de notre corpus de texte\n",
    "en_tokens = [token.text for token in en_doc]\n",
    "en_vocabulary_set= set(en_tokens)\n",
    "en_vocab_size = len(en_vocabulary_set)\n",
    "print(en_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zIwlGhNDykzn",
    "outputId": "899eaeec-4f4b-4cdd-ae8c-efef95b74cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12643\n"
     ]
    }
   ],
   "source": [
    "# Même chose pour le français \n",
    "fr_tokens = [token.text for token in fr_doc]\n",
    "fr_vocabulary_set= set(fr_tokens)\n",
    "fr_vocab_size = len(fr_vocabulary_set)\n",
    "print(fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "XUjLaIjB0e-e",
    "outputId": "bff2e844-b3e4-40d1-ef87-a96de1997d4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " 'Reading',\n",
       " 'books',\n",
       " 'is',\n",
       " 'my',\n",
       " 'hobby',\n",
       " '.',\n",
       " '<end>',\n",
       " '<start>',\n",
       " 'Tom']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "-6miVbPY0xZw",
    "outputId": "1d758ed3-0e11-40b4-98da-33aab6bf8a89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pool',\n",
       " 'inferior',\n",
       " 'oak',\n",
       " 'puddle',\n",
       " 'winks',\n",
       " 'birthday',\n",
       " 'Nice',\n",
       " 'screamed',\n",
       " 'rapid',\n",
       " 'anybody']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in en_vocabulary_set][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Uy_ou60_3X4X"
   },
   "outputs": [],
   "source": [
    "# Création d'un id pour chacun des tokens\n",
    "all_en_tokens = {}\n",
    "for i,en_token in enumerate(en_vocabulary_set):\n",
    "    all_en_tokens[en_token] = i+1 # On prend à i+1 pour laisser la valeur 0 pour la création des padded_sequences\n",
    "\n",
    "all_fr_tokens = {}\n",
    "for i, fr_token in enumerate(fr_vocabulary_set):\n",
    "    all_fr_tokens[fr_token] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de distance pour évaluer la proximité entre deux mots.\n",
    "def distance_edition(m1,m2,cache=None):\n",
    "    m1 = \"  \" + m1 + \"  \"\n",
    "    m2 = \"  \" + m2 + \"  \"\n",
    "    dist = {}\n",
    "    dist[-1,-1] = 0\n",
    "    for i in range(0,len(m1)):\n",
    "        dist[i,-1] = i\n",
    "    for j in range(0,len(m2)):\n",
    "        dist[-1,j] = j\n",
    "\n",
    "    for i, c in enumerate(m1):\n",
    "        for j, d in enumerate(m2):\n",
    "            d1 = dist[i-1,j] + 1 # insertion\n",
    "            d2 = dist[i,j-1] + 1 # suppression\n",
    "            x = 0 if c == d else 1\n",
    "            d3 = dist[i-1,j-1] + x\n",
    "            dist[i,j] = min(d1,d2,d3)\n",
    "    return dist[len(m1)-1, len(m2)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui renvoie pour un mot donnés, le mot le plus proche parmi ceux qui existent pour le modèle\n",
    "def closest_existing_word(string) :\n",
    "    lst = []\n",
    "    for i in all_fr_tokens.keys() :\n",
    "        lst.append((distance_edition(string, i), i))\n",
    "    return (min(lst)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de fonction qui vont créer un vecteur d'indices pour chacune des séquences de tokens\n",
    "def en_tokens_to_index(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(all_en_tokens[closest_existing_word(token.text)])\n",
    "    return indices\n",
    "\n",
    "def fr_tokens_to_index(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(all_fr_tokens[closest_existing_word(token.text)])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ZFIfUb1i7ia3"
   },
   "source": [
    "# Outdated, a suppr lors du prochain entrainement si tout est ok\n",
    "def en_tokens_to_index(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(all_en_tokens[token.text])\n",
    "    return indices\n",
    "\n",
    "def fr_tokens_to_index(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(all_fr_tokens[token.text])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CA8Gc2Yw9abI"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2d40970888c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Transformation des tokens en indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fr_indices\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fr_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfr_tokens_to_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"en_indices\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"en_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0men_tokens_to_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-2d40970888c4>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Transformation des tokens en indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fr_indices\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fr_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfr_tokens_to_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"en_indices\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"en_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0men_tokens_to_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-ff15a02c7c05>\u001b[0m in \u001b[0;36mfr_tokens_to_index\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_fr_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclosest_existing_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-0997d832d811>\u001b[0m in \u001b[0;36mclosest_existing_word\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_fr_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mlst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance_edition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-be96c5f23395>\u001b[0m in \u001b[0;36mdistance_edition\u001b[1;34m(m1, m2, cache)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0md1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# insertion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0md2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# suppression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0md\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0md3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Transformation des tokens en indices\n",
    "doc[\"fr_indices\"] = doc[\"fr_tokens\"].apply(lambda x: fr_tokens_to_index(x))\n",
    "doc[\"en_indices\"] = doc[\"en_tokens\"].apply(lambda x: en_tokens_to_index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "rIWHfQsj-Yn8",
    "outputId": "14eeeb6a-c69f-4671-b462-b3d236d019a5"
   },
   "outputs": [],
   "source": [
    "doc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgFwiD7TQXHg"
   },
   "outputs": [],
   "source": [
    "# Création d'une fonction qui va compter la longueur maximum d'une phrase\n",
    "def max_len(lines):\n",
    "    return max(len(line) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PppvMC4K7J7f"
   },
   "outputs": [],
   "source": [
    "# Application de la fonction sur les tokens français et anglais \n",
    "fr_max_len = max_len(doc['fr_indices'].to_list())\n",
    "en_max_len = max_len(doc['en_indices'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vSi2j-6I7K5z",
    "outputId": "4ee63423-3048-40d8-bb9b-08a09ecaacdd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Utilisation de Keras pour créer des séquences de tokens de la même longueur\n",
    "\n",
    "padded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"fr_indices\"], maxlen=fr_max_len, padding=\"post\")\n",
    "padded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"en_indices\"], maxlen=en_max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "yEBeXGQyvaeR",
    "outputId": "515c7fdd-c511-473e-90ec-9daf4b1ab9f5"
   },
   "outputs": [],
   "source": [
    "padded_en_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueIigsED1nWq"
   },
   "outputs": [],
   "source": [
    "# Création de variables que l'on va réutiliser pour nos modèles\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = int(0.7*len(doc)/BATCH_SIZE)\n",
    "BUFFER_SIZE = TAKE_SIZE * BATCH_SIZE\n",
    "steps_per_epoch = TAKE_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = fr_vocab_size\n",
    "vocab_tar_size = en_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZOn0SAFB5eO"
   },
   "outputs": [],
   "source": [
    "# Create a tensorflow dataset complet\n",
    "tf_ds = tf.data.Dataset.from_tensor_slices((padded_fr_indices, padded_en_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0aIpW5cZmnh"
   },
   "outputs": [],
   "source": [
    "# Shuffle & Batch\n",
    "tf_ds = tf_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCrDIAkMZPUM"
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "train_data = tf_ds.take(TAKE_SIZE)\n",
    "test_data = tf_ds.skip(TAKE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "1EmEescP6jSL",
    "outputId": "a0874a6d-11e9-481a-ecb3-943d1d660cdf"
   },
   "outputs": [],
   "source": [
    "input_text, output_text = next(iter(train_data))\n",
    "print(input_text.numpy().shape)\n",
    "print(output_text.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbbOi7sYVDU2"
   },
   "outputs": [],
   "source": [
    "# Encoder \n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "4cmD6jUP74Rx",
    "outputId": "96c40153-148f-445e-b2c6-e8cb4ab517fb"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size +1, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# Echantillon d'output\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(input_text, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq2nP5l_76LS"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # Ceci est fait pour pour calculer notre score \"d'attention\"\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # On obtient 1 sur le dernier axe car on applique le score à self.V\n",
    "    # La shape du tenseur avant que l'on applique self.V est (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "KetnxQvd8bF0",
    "outputId": "93909cb7-82c3-4d29-e7b3-03bf105ca6cc"
   },
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ruo6sIk98eLs"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Utilisé pour attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape après embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape après concaténation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # Passage du vecteur concaténé à la couche GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jPW5WmrY8hyT",
    "outputId": "229c53a9-c5d8-45a1-bc27-616f5e67c640"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size + 1, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2h8jDTT8wHq"
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ldPbErh8j1x"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBFavAT78w25"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8V4m-De84om"
   },
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUAgTA4-8zsX"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([all_en_tokens[\"<start>\"]] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XgoAc_Sd85tt",
    "outputId": "8846e050-3fe4-4871-b693-0703b2606330"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = TAKE_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_data.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  \n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LtkdE_Z2IaOH",
    "outputId": "1b4bcb66-c0cf-452e-bc5a-0ef7ee83c780"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "a7O5bIzk-S4V",
    "outputId": "5b106b36-de96-4175-e52e-5dc5884730c0"
   },
   "outputs": [],
   "source": [
    "for example, label in test_data.take(10):\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    input_t = example[0]\n",
    "    output_label = label[0]\n",
    "    enc_out, enc_hidden = encoder(tf.expand_dims(input_t, axis=0), hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([all_en_tokens[\"<start>\"]], 0)\n",
    "\n",
    "    result = \"\"\n",
    "    for t in range(padded_fr_indices.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                          dec_hidden,\n",
    "                                                          enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        corresponding_word = [word for word, id in all_en_tokens.items() if id==predicted_id]\n",
    "        result += corresponding_word[0] + \" \"\n",
    "\n",
    "        if corresponding_word[0] == '<end>':\n",
    "            break\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    input_sentence = \"\"\n",
    "    for token_id in input_t:\n",
    "        if token_id==0:\n",
    "            break\n",
    "    \n",
    "        corresponding_word = [word for word, id in all_fr_tokens.items() if id==token_id]\n",
    "        input_sentence += corresponding_word[0] + \" \"\n",
    "        if corresponding_word[0] == \"<end>\":\n",
    "            break\n",
    "\n",
    "        true_translation = \"\"\n",
    "    for token_id in output_label:\n",
    "        if token_id==0:\n",
    "            break\n",
    "        corresponding_word = [word for word, id in all_en_tokens.items() if id==token_id]\n",
    "        true_translation += corresponding_word[0] + \" \"\n",
    "        if corresponding_word[0] == \"<end>\":\n",
    "            break \n",
    "\n",
    "    true_translation = true_translation[8:]\n",
    "    print(\"French sentence: {}\".format(input_sentence))\n",
    "    print(\"True translation: {}\".format(true_translation))\n",
    "    print(\"Modl translation: {}\".format(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNZMyM88_JPs"
   },
   "source": [
    "# Application concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = input()\n",
    "new_df = pd.DataFrame({'en': [\"An English sentence\"], 'fr' : [phrase]})\n",
    "new_df.iloc[:, 0] = new_df.iloc[:, 0].apply(lambda x: begin_end_sentence(x))\n",
    "new_fr_corpus = \" \".join(new_df.iloc[:, 1].to_list())\n",
    "new_en_corpus = \" \".join(new_df.iloc[:, 0].to_list())\n",
    "new_df[\"fr_tokens\"] = new_df.iloc[:, 1].apply(lambda x: nlp_fr.tokenizer(x))\n",
    "new_df[\"en_tokens\"] = new_df.iloc[:, 0].apply(lambda x: nlp_en.tokenizer(x))\n",
    "new_df[\"fr_indices\"] = new_df[\"fr_tokens\"].apply(lambda x: fr_tokens_to_index(x))\n",
    "new_df[\"en_indices\"] = new_df[\"en_tokens\"].apply(lambda x: en_tokens_to_index(x))\n",
    "new_padded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(new_df[\"fr_indices\"], maxlen=fr_max_len, padding=\"post\")\n",
    "new_padded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(new_df[\"en_indices\"], maxlen=en_max_len, padding=\"post\")\n",
    "new_tf_ds = tf.data.Dataset.from_tensor_slices((new_padded_fr_indices, new_padded_en_indices))\n",
    "new_tf_ds = new_tf_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)\n",
    "test = new_tf_ds.skip(0)\n",
    "for example, label in test:\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    input_t = example[0]\n",
    "    output_label = label[0]\n",
    "    enc_out, enc_hidden = encoder(tf.expand_dims(input_t, axis=0), hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([all_en_tokens[\"<start>\"]], 0)\n",
    "\n",
    "    result = \"\"\n",
    "    for t in range(new_padded_fr_indices.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                          dec_hidden,\n",
    "                                                          enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        corresponding_word = [word for word, id in all_en_tokens.items() if id==predicted_id]\n",
    "        result += corresponding_word[0] + \" \"\n",
    "\n",
    "        if corresponding_word[0] == '<end>':\n",
    "            break\n",
    "    \n",
    "    # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    input_sentence = \"\"\n",
    "    for token_id in input_t:\n",
    "        if token_id==0:\n",
    "            break\n",
    "    \n",
    "        corresponding_word = [word for word, id in all_fr_tokens.items() if id==token_id]\n",
    "        input_sentence += corresponding_word[0] + \" \"\n",
    "        if corresponding_word[0] == \"<end>\":\n",
    "            break\n",
    "\n",
    "    true_translation = true_translation[8:]\n",
    "    print(\"French sentence: {}\".format(input_sentence))\n",
    "    print(\"Model translation: {}\".format(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corresponding_word = [word for word, id in all_en_tokens.items() if id==predicted_id]\n",
    "print(corresponding_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_en_tokens.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t chesterish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_edition(m1,m2,cache=None):\n",
    "    m1 = \"  \" + m1 + \"  \"\n",
    "    m2 = \"  \" + m2 + \"  \"\n",
    "    dist = {}\n",
    "    dist[-1,-1] = 0\n",
    "    for i in range(0,len(m1)):\n",
    "        dist[i,-1] = i\n",
    "    for j in range(0,len(m2)):\n",
    "        dist[-1,j] = j\n",
    "\n",
    "    for i, c in enumerate(m1):\n",
    "        for j, d in enumerate(m2):\n",
    "            d1 = dist[i-1,j] + 1 # insertion\n",
    "            d2 = dist[i,j-1] + 1 # suppression\n",
    "            x = 0 if c == d else 1\n",
    "            d3 = dist[i-1,j-1] + x\n",
    "            dist[i,j] = min(d1,d2,d3)\n",
    "    return dist[len(m1)-1, len(m2)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_edition(\"Checks\", \"checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_tokens_to_index(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(all_fr_tokens[closest_existing_word(token.text)])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_existing_word(string) :\n",
    "    lst = []\n",
    "    for i in all_fr_tokens.keys() :\n",
    "        lst.append((distance_edition(string, i), i))\n",
    "    return (min(lst)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    lst = []\n",
    "    for i in all_fr_tokens.keys() :\n",
    "        lst.append((distance_edition(\"il\", i), i))\n",
    "    lst.sort()\n",
    "    lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fr_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"fr_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Traductions 2.0 - SOLUTION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
